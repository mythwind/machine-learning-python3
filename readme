
机器学习
    机器学习就是把无序的数据转换成有用的信息

k-1. 近邻算法：采用测量不同特征值之间的距离方法进行分类
    工作原理：存在一个样本数据集合，也称作为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一个数据与所属分类的对应关系。
    输入没有标签的新数据后，将新的数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本最相似数据(最近邻)的分类标签。
    一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。
    k-近邻算法步骤如下：
        计算已知类别数据集中的点与当前点之间的距离；
        按照距离递增次序排序；
        选取与当前点距离最小的k个点；
        确定前k个点所在类别的出现频率；
        返回前k个点所出现频率最高的类别作为当前点的预测分类。
    一般流程：
        (1) 收集数据
        (2) 准备数据
        (3) 分析数据
        (4) 训练算法
        (5) 测试算法
        (6) 使用算法
kNN算法的优缺点
优点:
    简单好用，容易理解，精度高，理论成熟，既可以用来做分类也可以用来做回归；
    可用于数值型数据和离散型数据；
    训练时间复杂度为O(n)；无数据输入假定；
    对异常值不敏感。

缺点:
    计算复杂性高；空间复杂性高；
    样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；
    一般数值很大的时候不用这个，计算量太大。但是单个样本又不能太少，否则容易发生误分。
    最大的缺点是无法给出数据的内在含义。


2. 决策树(Decision Tree）:
    决策树是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。
    决策树是一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果。

    熵定义为信息的期望值。在信息论与概率统计中，熵是表示随机变量不确定性的度量。
    信息定义：l(xi) = - log2p(xi)
    其中p(xi)是选择该分类的概率。
    熵定义：H = -sum(p(xi)log2p(xi))

    当熵中的概率由数据估计(特别是最大似然估计)得到时，所对应的熵称为经验熵(empirical entropy)。
    条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性，随机变量X给定的条件下随机变量Y的条件熵(conditional entropy) H(Y|X)，定义X给定条件下Y的条件概率分布的熵对X的数学期望：
    H(Y|X) = sum(PiH(Y|X=xi))

    ID	年龄	有工作	房子	    信贷情况	类别(是否个给贷款)
    1	青年	否	    否	    一般	    否
    2	青年	否	    否	    好	    否
    3	青年	是   	否	    好	    是
    4	青年	是	    是	    一般	    是
    5	青年	否	    否	    一般	    否
    6	中年	否	    否	    一般	    否
    7	中年	否	    否	    好	    否
    8	中年	是	    是	    好	    是
    9	中年	否	    是	    非常好	是
    10	中年	否	    是	    非常好	是
    11	老年	否	    是	    非常好	是
    12	老年	否	    是	    好	    是
    13	老年	是	    否	    好	    是
    14	老年	是	    否	    非常好	是
    15	老年	否	    否	    一般	    否

    构造决策树是很耗时的任务，即使处理很小的数据集，如前面的样本数据，也要花费几秒的时间，如果数据集很大，将会耗费很多计算时间。
    然而用创建好的决策树解决分类问题，则可以很快完成。因此，为了节省计算时间，最好能够在每次执行分类时调用已经构造好的决策树。
    为了解决这个问题，需要使用Python模块pickle序列化对象。序列化对象可以在磁盘上保存对象，并在需要的时候读取出来。

决策树的一些优点：
    易于理解和解释，决策树可以可视化。
    几乎不需要数据预处理。其他方法经常需要数据标准化，创建虚拟变量和删除缺失值。决策树还不支持缺失值。
    使用树的花费（例如预测数据）是训练数据点(data points)数量的对数。
    可以同时处理数值变量和分类变量。其他方法大都适用于分析一种变量的集合。
    可以处理多值输出变量问题。
    使用白盒模型。如果一个情况被观察到，使用逻辑判断容易表示这种规则。相反，如果是黑盒模型（例如人工神经网络），结果会非常难解释。
    即使对真实模型来说，假设无效的情况下，也可以较好的适用。
决策树的一些缺点：
    决策树学习可能创建一个过于复杂的树，并不能很好的预测数据。也就是过拟合。修剪机制（现在不支持），设置一个叶子节点需要的最小样本数量，或者数的最大深度，可以避免过拟合。
    决策树可能是不稳定的，因为即使非常小的变异，可能会产生一颗完全不同的树。这个问题通过decision trees with an ensemble来缓解。
    学习一颗最优的决策树是一个NP-完全问题under several aspects of optimality and even for simple concepts。因此，传统决策树算法基于启发式算法，例如贪婪算法，即每个节点创建最优决策。这些算法不能产生一个全家最优的决策树。对样本和特征随机抽样可以降低整体效果偏差。
    概念难以学习，因为决策树没有很好的解释他们，例如，XOR, parity or multiplexer problems.
    如果某些分类占优势，决策树将会创建一棵有偏差的树。因此，建议在训练之前，先抽样使样本均衡。


3. 朴素贝叶斯算法
    朴素贝叶斯算法是有监督的学习算法，解决的是分类问题，如客户是否流失、是否值得投资、信用等级评定等多分类问题。
    该算法的优点在于简单易懂、学习效率高、在某些领域的分类问题中能够与决策树、神经网络相媲美。
    但由于该算法以自变量之间的独立（条件特征独立）性和连续变量的正态性假设为前提，就会导致算法精度在某种程度上受影响。

    贝叶斯决策理论:选择具有最高概率的决策。
    条件概率(Condittional probability)，就是指在事件B发生的情况下，事件A发生的概率，用P(A|B)来表示。
    条件概率：P(A|B) = P(B|A)P(A) / P(B)
    全概率  ：P(B) = P(B|A)P(A) + P(B|A')P(A')


    我们把P(A)称为"先验概率"（Prior probability），即在B事件发生之前，我们对A事件概率的一个判断。
    P(A|B)称为"后验概率"（Posterior probability），即在B事件发生之后，我们对A事件概率的重新评估。
    P(B|A)/P(B)称为"可能性函数"（Likelyhood），这是一个调整因子，使得预估概率更接近真实概率。

    后验概率　＝　先验概率 ｘ 调整因子
    这就是贝叶斯推断的含义。我们先预估一个"先验概率"，然后加入实验结果，看这个实验到底是增强还是削弱了"先验概率"，由此得到更接近事实的"后验概率"。

    这就是贝叶斯分类器的基本方法：在统计资料的基础上，依据某些特征，计算各个类别的概率，从而实现分类。


朴素贝叶斯推断的一些优点：
    生成式模型，通过计算概率来进行分类，可以用来处理多分类问题。
    对小规模的数据表现很好，适合多分类任务，适合增量式训练，算法也比较简单。
朴素贝叶斯推断的一些缺点：
    对输入数据的表达形式很敏感。
    由于朴素贝叶斯的“朴素”特点，所以会带来一些准确率上的损失。
    需要计算先验概率，分类决策存在错误率。

4. Logistic回归（目前不太理解，运算规则）
    优点：计算代价不高，易于理解和实现。
    缺点：容易欠拟合，分类精度可能不高。
    适用数据类型：数值型和标称型数据

    假设现在有一些数据点，我们利用一条直线对这些点进行拟合(该线称为最佳拟合直线)，这个拟合过程就称作为回归，
    Logistic回归进行分类的主要思想是：根据现有数据对分类边界线建立回归公式，以此进行分类。其实，Logistic本质上是一个基于条件概率的判别模型(Discriminative Model)。
    hθ(x) = g(θᵀx) = 1 / (1 + e^(-θᵀx))
    其中X是变量，θ是参数，由于是多维，所以写成了向量的形式，也可以看作矩阵。θT表示矩阵θ的转置，即行向量变成列向量。θTX是矩阵乘法。（高数结合线性代数的知识）

    梯度上升算法：

    我们每次更新回归系数(最优参数)的时候，能不能不用所有样本呢？一次只用一个样本点去更新回归系数(最优参数)？这样就可以有效减少计算量了，这种方法就叫做随机梯度上升算法。





